y = "groupe",
facet_rows = vars(.question),
facet_label_wrap = 15
)+
guides(fill = guide_legend(nrow = 2)) # selon les questions
library(tidyverse)
ggplot(iris) +
aes(x = Petal.Length, y = Petal.Width) +
geom_point(colour = "blue", alpha = .25) +
labs(x = "Longueur", y = "Largeur") +
theme_light()
data(iris)
# charger les packages
library(tidyverse)
# charger les données
data("iris")
ggplot(iris) +
aes(x = Petal.Length, y = Petal.Width) +
geom_point(colour = "blue", alpha = .25) +
labs(x = "Longueur", y = "Largeur") +
theme_light()
# Résumé des données
str(iris)
head(iris)
summary(iris)
# visualisation graphique
ggplot(iris) +
aes(x = Petal.Length, y = Petal.Width) +
geom_point(colour = "blue", alpha = .25) +
geom_smooth(method = "lm") +
labs(x = "Longueur", y = "Largeur") +
theme_light()
# charger les données
data("mtcars")
# Résumé des données
str(mtcars)
head(mtcars)
summary(mtcars$mpg, mtcars$drat)
summary(mtcars$mpg)
# visualisation graphique
ggplot(mtcars) +
aes(x = mpg, y = drat) +
geom_point(colour = "blue", alpha = .25) +
geom_smooth(method = "lm") +
labs(x = "Longueur", y = "Largeur") +
theme_light()
# visualisation graphique
ggplot(mtcars) +
aes(x = mpg, y = wt) +
geom_point(colour = "blue", alpha = .25) +
geom_smooth(method = "lm") +
labs(x = "Longueur", y = "Largeur") +
theme_light()
# visualisation graphique
ggplot(mtcars) +
aes(x = wt, y = mpg) +
geom_point(colour = "blue", alpha = .25) +
geom_smooth(method = "lm") +
labs(x = "Longueur", y = "Largeur") +
theme_light()
# visualisation graphique
ggplot(mtcars) +
aes(x = wt, y = mpg) +
geom_point(colour = "blue", alpha = .25) +
geom_smooth(method = "lm") +
labs(x = "Poids de la voiture", y = "Consommation du carburant") +
theme_light()
# régression linéaire
mod <- lm(mpg ~ wt, data = mtcars)
mod
ggplot(mtcars) +
aes(x = wt, y = mpg) +
geom_point(colour = "blue", alpha = .25) +
geom_abline(
intercept = mod$coefficients[1],
slope = mod$coefficients[2],
linewidth = 1,
colour = "red"
) +
geom_vline(xintercept = 0, linewidth = 1, linetype = "dotted") +
labs(x = "Poids de la voiture", y = "Consommation du carburant") +
expand_limits(x = 0, y = -1) +
theme_light()
ggplot(mtcars) +
aes(x = wt, y = mpg) +
geom_point(colour = "blue", alpha = .25) +
geom_abline(
intercept = mod$coefficients[1],
slope = mod$coefficients[2],
linewidth = 1,
colour = "red"
) +
geom_vline(xintercept = 0, linewidth = 1, linetype = "dotted") +
labs(x = "Poids de la voiture", y = "Consommation du carburant") +
expand_limits(x = -1, y = -1) +
theme_light()
ggplot(mtcars) +
aes(x = wt, y = mpg) +
geom_point(colour = "blue", alpha = .25) +
geom_abline(
intercept = mod$coefficients[1],
slope = mod$coefficients[2],
linewidth = 1,
colour = "red"
) +
geom_vline(xintercept = 0, linewidth = 1, linetype = "dotted") +
labs(x = "Poids de la voiture", y = "Consommation du carburant") +
theme_light()
library(gtsummary)
tbl_regression(mod, intercept = TRUE)
# Résumé des données
str(mtcars)
# régression linéaire avec variable explicative catégorielle
mod2 <- lm(mpg ~ carb, data = mtcars)
mod2
tbl_regression(mod2, intercept = TRUE)
str(mtcars)
# régression linéaire avec plusieurs variables explicatives
mod2 <- lm(mpg ~ carb + gear + cyl + disp + hp + drat + qsec, data = mtcars)
tbl_regression(mod2, intercept = TRUE)
library(ggstats)
# importance des variables
ggcoef_model(mod2)
# charger les packages
library(tidyverse)
# charger les données
data(hdv2003, package = "questionr")
library(labelled)
# Vérification de la qualité des données
look_for(hdv2003, "sport")
look_for(hdv2003, "sexe")
mutate(hdv2003, fct_relevel(sexe = sexe, "Femme"))
hdv2003 <- hdv2003 |>
mutate(sexe = sexe |> fct_relevel("Femme"))
freq(hdv2003$sexe)
library(questionr)
freq(hdv2003$sexe)
look_for(hdv2003, "age", "heures")
## Création de la variable tranche d'âge
hdv2003 <- hdv2003 |>
mutate(
groupe_ages = age |>
cut(
c(18, 25, 45, 65, 99),
right = FALSE,
include.lowest = TRUE,
labels = c("18-24 ans", "25-44 ans",
"45-64 ans", "65 ans et plus")
)
)
freq(hdv2003$groupe_ages)
hdv2003 <- hdv2003 |>
mutate(
etudes = nivetud |>
fct_recode(
"Primaire" = "N'a jamais fait d'etudes",
"Primaire" = "A arrete ses etudes, avant la derniere annee d'etudes primaires",
"Primaire" = "Derniere annee d'etudes primaires",
"Secondaire" = "1er cycle",
"Secondaire" = "2eme cycle",
"Technique / Professionnel" = "Enseignement technique ou professionnel court",
"Technique / Professionnel" = "Enseignement technique ou professionnel long",
"Supérieur" = "Enseignement superieur y compris technique superieur"
)
)
freq(d$etudes)
freq(hdv2003$etudes)
### conserver les valeurs manquantes car non négligeable
hdv2003$etudes <- fct_na_value_to_level(hdv2003$etudes, "Non documenté")
hdv2003 <- hdv2003 |>
set_variable_labels(
sport = "Pratique un sport ?",
sexe = "Sexe",
groupe_ages = "Groupe d'âges",
etudes = "Niveau d'études",
relig = "Rapport à la religion",
heures.tv = "Heures de télévision / jour"
)
library(gtsummary)
hdv2003 |>
tbl_summary(
by = sport,
include = c(sexe, groupe_ages, etudes, relig, heures.tv)
) |>
add_overall(last = TRUE) |>
add_p() |>
bold_labels() |>
modify_spanning_header(
update = all_stat_cols() ~ "**Pratique un sport ?**"
)
mod <- glm(
sport ~ sexe + groupe_ages + etudes + relig + heures.tv,
family = binomial,
data = hdv2003
)
mod_binomiale <- glm(
sport ~ sexe + groupe_ages + etudes + relig + heures.tv,
family = binomial,
data = hdv2003
)
bold_labels(tbl_regression(mod_binomiale, intercept = TRUE))
hdv2003[1, ]
# Visualisation graphique
ggcoef_model(mod_binomiale, exponentiate = TRUE)
ggcoef_table(mod_binomiale, exponentiate = TRUE)
mod_binomiale |>
tbl_regression() |>
add_significance_stars() |>
modify_column_hide(c("ci", "p.value")) |>
modify_column_unhide("std.error") |>
bold_labels()
library(survey)
# installer un package
install.packages("survey")
# charger un package
library(survey)
# échantillonnage
## aléatoire simple
p_iris <- svydesign(
ids = ~ 1,
data = iris
)
p_iris
data("Titanic")
library(labelled)
# nettoyage des données
look_for(Titanic)
# nettoyage des données
titanic <- as_tibble(Titanic)
look_for(Titanic)
library(dplyr)
# nettoyage des données
titanic <- as_tibble(Titanic)
look_for(Titanic)
titanic
Titanic
look_for(titanic)
## simplement pondéré
p_titanic <- svydesign(
ids = ~ 1,
data = titanic,
weights = ~ n
)
data("api", package = "survey")
## échantillon stratifié
p_strates <- survey::svydesign(
id = ~ 1,
strata = ~ stype,
weights = ~ pw,
data = apistrat
)
p_strates
## enquête en grappes à 1 degré
p_grappes <- svydesign(
id = ~ dnum,
weights = ~ pw,
data = apiclus1
)
## enquête en grappes à 2 degrés (les 2 niveaux sont donnés par les variables dnum et snum)
p_grappes2 <- svydesign(
id = ~ dnum + snum,
fpc = ~ fpc1 + fcp2,
data = apiclus2
)
weights(p_grappes2)
weights(p_grappes2)
## enquête en grappes à 2 degrés (les 2 niveaux sont donnés par les variables dnum et snum)
p_grappes2 <- svydesign(
id = ~ dnum + snum,
fpc = ~ fpc1 + fpc2,
data = apiclus2
)
weights(p_grappes2)
apiclus2
# installer les packages
install.packages("srvyr")
# charger les packages
library(srvyr)
# charger les données
data("iris")
# échantillonnage aléatoire simple
t_iris <- as_survey_design(iris)
class(t_iris)
library(dplyr)
# nettoyage des données
titanic <- as_tibble(Titanic)
t_titanic <- as_survey_design(titanic, weights = n)
data("api", package = "survey")
data("api", package = "survey")
## échantillon stratifié
t_strates <- as_survey_design(apistrat,
strata = stype,
weights = pw)
t_strates
## échantillon en grappes
### échantillon en grappes à 1 degré
t_grappes <- as_survey_design(apiclus1,
id = dnum,
weights = pw)
### échantillon en grappes à 2 degré``
t_grappes2 <- as_survey_design(apiclus2,
id = c(dnum, snum),
fpc = c(fpc1, fpc2))
t_grappes2
# charger les données
library(tidyverse)
library(nycflights13)
# charger les données
data(flights)
data(airports)
data(airlines)
airlines_flights |>
select(month, day, carrier, name) |>
head(10)
### Cibler des variables comme clé
flights |>
left_join(airports, by = c("origin" = "faa")) |>
head(10)
## dataframe "personne"
personnes <- tibble(
nom = c("Sylvie", "Sylvie", "Monique", "Gunter", "Rayan", "Rayan"),
voiture = c("Twingo", "Ferrari", "Scenic", "Lada", "Twingo", "Clio")
)
## dataframe "voiture"
voitures <- tibble(
voiture = c("Twingo", "Ferrari", "Clio", "Lada", "208"),
vitesse = c("140", "280", "160", "85", "160")
)
## Right join
right_join(personnes, voitures, by = "voiture")
## Inner join
inner_join(personnes, voitures, by = "voiture")
## Full join
full_join(personnes, voitures, by = "voiture")
## Anti join
anti_join(personnes, voitures, by = "voiture")
## Merge
merge(personnes, voitures, by = "voiture")
setwd("~/Documents/R-programming/Advanced Statistics/Ensemble_learning/random_forest")
data <- read.csv("diabetes.csv")
head(data)
str(data)
data$Outcome <- as.factor(data$Outcome)
str(data)
summary(data)
library(caret)
install.packages("caret")
library(caret)
# Définir un seed pour la reproductibilité
set.seed(123)
# Définir un seed pour la reproductibilité
set.seed(123)
# créer un indice pour partitionner les données (80% entraînement, 20% test)
trainIndex <- createDataPartition(data$Outcome, p=0.8, list = FALSE)
trainIndex
# Diviser les données en ensembles d'entraînement et de test
dataTrain <- data[trainIndex, ]
dataTest <- data[-trainIndex, ]
# Afficher les dimensions des ensembles d'entraînement et de test
cat("Dimensions de l'ensemble d'entraînement : ", dim(dataTrain), "\n")
cat("Dimensions de l'ensemble de test : ", dim(dataTest), "\n")
install.packages("rpart")
install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
library(rpart)
library(rpart.plot)
library(caret)
data <- read.csv("diabetes.csv")
head(data)
str(data) # informations sur les variables
data$Outcome <- as.factor(data$Outcome) # formattage de la variable 'Outcome'
str(data)
summary(data) # résultats statistiques selon le type de variables
# Définir un seed pour la reproductibilité
set.seed(123)
# créer un indice pour partitionner les données (80% entraînement, 20% test)
trainIndex <- createDataPartition(data$Outcome, p=0.8, list = FALSE)
# Diviser les données en ensembles d'entraînement et de test
dataTrain <- data[trainIndex, ]
dataTest <- data[-trainIndex, ]
# Afficher les dimensions des ensembles d'entraînement et de test
cat("Dimensions de l'ensemble d'entraînement : ", dim(dataTrain), "\n")
cat("Dimensions de l'ensemble de test : ", dim(dataTest), "\n")
# Construire l'arbre de décision
treeModel <- rpart(Outcome ~ ., data = dataTrain, method = "class")
summary(treeModel)
# Visualiser l'arbre de décision
rpart.plot(treeModel, main = "Arbre de Décision - Modèle de Diabète")
printcp(treeModel)
# Tracer le graphique du paramètre de complexité de l'arbre
plotcp(treeModel)
# Choisir le paramètre de complexité optimal (celui avec la plus petite erreur relative)
optimalCp <- treeModel$cptable[which.min(treeModel$cptable[, "xerror"]), "CP"]
# Élaguer l'arbre en utilisant le paramètre de complexité optimal
prunedTreeModel <- prune(treeModel, cp = optimalCp)
# Visualiser l'abre élagué
rpart.plot(prunedTreeModel, main = "Arbre de Décision Élagué - Modèle de Diabète")
optimalCp
# Visualiser l'abre élagué
rpart.plot(prunedTreeModel, main = "Arbre de Décision Élagué - Modèle de Diabète")
# Visualiser l'abre élagué
rpart.plot(prunedTreeModel, main = "Arbre de Décision Élagué - Modèle de Diabète")
# Choisir le paramètre de complexité optimal (celui avec la plus petite erreur relative)
optimalCp <- treeModel$cptable[which.min(treeModel$cptable[, "xerror"]), "CP"]
# Élaguer l'arbre en utilisant le paramètre de complexité optimal
prunedTreeModel <- prune(treeModel, cp = optimalCp)
# Visualiser l'abre élagué
rpart.plot(prunedTreeModel, main = "Arbre de Décision Élagué - Modèle de Diabète")
library(ROCR)
install.packages("ROCR")
library(ROCR)
# Visualiser l'abre élagué
prune.plot(prunedTreeModel, main = "Arbre de Décision Élagué - Modèle de Diabète")
# Visualiser l'abre élagué
prune.rpart(prunedTreeModel, main = "Arbre de Décision Élagué - Modèle de Diabète")
# Visualiser l'abre élagué
prune.rpart(prunedTreeModel, main = "Arbre de Décision Élagué - Modèle de Diabète")
# Visualiser l'abre élagué
rpart.plot(prunedTreeModel, main = "Arbre de Décision Élagué - Modèle de Diabète")
# Réaliser des prédictions probabilistes sur l'ensemble de test avec l'arbre élagué
probPredictions <- predict(prunedTreeModel, dataTest, type = "prob")[,2]
# Utiliser ROCR pour tracer la courbe ROC
pred <- prediction(probPredictions, dataTest$Outcome)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
# Tracer la courbe ROC
plot(perf, main = "Courbe ROC - Arbre de Décision Élagué", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")
# Calculer et afficher l'AUC
auc <- performance(pred, measure = "auc")
aucValue <- auc@v.values[[1]]
# Utiliser ROCR pour tracer la courbe ROC
pred <- prediction(probPredictions, dataTest$Outcome)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
# Tracer la courbe ROC
plot(perf, main = "Courbe ROC - Arbre de Décision Élagué", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")
# Calculer et afficher l'AUC
auc <- performance(pred, measure = "auc")
aucValue <- auc@y.values[[1]]
cat("AUC :", aucValue, "\n")
install.packages("randomForest")
library(randomForest)
install.packages("randomForest")
library(randomForest)
data <- read.csv("diabetes.csv")
head(data)
str(data) # informations sur les variables
data$Outcome <- as.factor(data$Outcome) # formattage de la variable 'Outcome'
str(data)
summary(data) # résultats statistiques selon le type de variables
# Définir un seed pour la reproductibilité
set.seed(123)
# créer un indice pour partitionner les données (80% entraînement, 20% test)
trainIndex <- createDataPartition(data$Outcome, p=0.8, list = FALSE)
install.packages("caret")
library(caret)
install.packages("caret")
library(rpart)
library(rpart.plot)
library(randomForest)
library(e1071)
library(ROCR)
library(caret)
data <- read.csv("diabetes.csv")
head(data)
str(data) # informations sur les variables
data$Outcome <- as.factor(data$Outcome) # formattage de la variable 'Outcome'
str(data)
summary(data) # résultats statistiques selon le type de variables
# Définir un seed pour la reproductibilité
set.seed(123)
# créer un indice pour partitionner les données (80% entraînement, 20% test)
trainIndex <- createDataPartition(data$Outcome, p=0.8, list = FALSE)
# Diviser les données en ensembles d'entraînement et de test
dataTrain <- data[trainIndex, ]
dataTest <- data[-trainIndex, ]
# Afficher les dimensions des ensembles d'entraînement et de test
cat("Dimensions de l'ensemble d'entraînement : ", dim(dataTrain), "\n")
cat("Dimensions de l'ensemble de test : ", dim(dataTest), "\n")
# Construire l'arbre de décision
treeModel <- rpart(Outcome ~ ., data = dataTrain, method = "class")
summary(treeModel) # afficher le résumé du modèle
# Visualiser l'arbre de décision
rpart.plot(treeModel, main = "Arbre de Décision - Modèle de Diabète")
# Étudier la complexité de l'arbre
printcp(treeModel)
# Tracer le graphique du paramètre de complexité de l'arbre
plotcp(treeModel)
# Choisir le paramètre de complexité optimal (celui avec la plus petite erreur relative)
optimalCp <- treeModel$cptable[which.min(treeModel$cptable[, "xerror"]), "CP"]
# Élaguer l'arbre en utilisant le paramètre de complexité optimal
prunedTreeModel <- prune(treeModel, cp = optimalCp)
# Visualiser l'abre élagué
rpart.plot(prunedTreeModel, main = "Arbre de Décision Élagué - Modèle de Diabète")
# Réaliser des prédictions probabilistes sur l'ensemble de test avec l'arbre élagué
probPredictions <- predict(prunedTreeModel, dataTest, type = "prob")[,2]
# Utiliser ROCR pour tracer la courbe ROC
pred <- prediction(probPredictions, dataTest$Outcome)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
# Tracer la courbe ROC
plot(perf, main = "Courbe ROC - Arbre de Décision Élagué", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")
# Calculer et afficher l'AUC
auc <- performance(pred, measure = "auc")
aucValue <- auc@y.values[[1]]
cat("AUC :", aucValue, "\n")
rfModel <- randomForest(Outcome ~ ., data = dataTrain, importance = TRUE)
print(rfModel) # Afficher un résumé du modèle
# Importance des variables
importance(rfModel)
varImpPlot(rfModel)
